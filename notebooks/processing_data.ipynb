{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-22T14:14:45.622151Z","iopub.execute_input":"2023-07-22T14:14:45.622595Z","iopub.status.idle":"2023-07-22T14:14:45.768144Z","shell.execute_reply.started":"2023-07-22T14:14:45.622555Z","shell.execute_reply":"2023-07-22T14:14:45.766742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef save_json(data,name):\n    with open(name+'.json', 'w',encoding='utf8') as f:\n        json.dump(data, f,ensure_ascii=False)\n        \ndef save_jsonl(data,name):\n    with open(name+'.jsonl', 'w') as f:\n        for entry in data:\n            json.dump(entry, f)\n            outfile.write('\\n')\n\ndef load_json(path):\n    f = open (path, \"r\")\n    data = json.loads(f.read())\n    f.close()\n    return data\n\ndef load_jsonl(path):\n    f = open(path,\"r\").readlines()\n    data=[]\n    for i in f:\n        d = json.loads(i)\n        data.append(d)\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-07-22T14:14:47.156464Z","iopub.execute_input":"2023-07-22T14:14:47.156900Z","iopub.status.idle":"2023-07-22T14:14:47.168297Z","shell.execute_reply.started":"2023-07-22T14:14:47.156865Z","shell.execute_reply":"2023-07-22T14:14:47.166603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset 1","metadata":{}},{"cell_type":"markdown","source":"In dataset-1 we have 20 .txt files. Each file has a word and corresponding label in each line. We will extract all of them and make lists. If the number of space separated parts and number of labels are not same we will skip those sentences. If a token contains a name or part of a name it is labeled 1 otherwise 0.","metadata":{}},{"cell_type":"code","source":"\nall_sen = []\nall_leb = []\nall_leb_per = []\n\nfor k in range(20):\n\n    f = open(\"/path/to/Dataset_1/\"+str(k+1)+\".txt\", \"r\")\n    data = f.readlines()\n    data[0]=data[0][1:]\n\n    i = 0\n\n    while(i<len(data)):\n\n        if data[i]==\"\\n\":\n            i = i+1\n            continue\n        else:\n            sen = \"\"\n            leb = []\n            leb_per = []\n            while(i<len(data) and data[i]!=\"\\n\"):\n                sen_leb = data[i].split(\"\\t\")\n                sen = sen +\" \"+sen_leb[0].replace(\" \",\"\")\n                if sen_leb[1][:-1][-3:]==\"PER\":\n                    leb_per.append(1)\n                else:\n                    leb_per.append(0)\n                leb.append(sen_leb[1][:-1])\n                i = i+1\n\n            all_leb_per.append(leb_per)\n            all_sen.append(sen[1:])\n            all_leb.append(leb)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-22T14:14:48.660107Z","iopub.execute_input":"2023-07-22T14:14:48.660484Z","iopub.status.idle":"2023-07-22T14:14:49.219786Z","shell.execute_reply.started":"2023-07-22T14:14:48.660454Z","shell.execute_reply":"2023-07-22T14:14:49.218325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset 2","metadata":{}},{"cell_type":"markdown","source":"In dataset-2 there is some errors in labeling. There are some sentences which contains multiple names but their labels are not given at same label. The sentence is repeated multiple times and each time the only one name label is given. First we will have to merge all of them. I","metadata":{}},{"cell_type":"code","source":"data_2 = load_jsonl(\"/path/to/Dataset_2.jsonl\")\n\n# Making a csv and groupig multiple labels\nall_sen_bad = [data_2[i][0] for i in range(len(data_2))]\nall_leb_bad = [data_2[i][1] for i in range(len(data_2))]\n\ndf = pd.DataFrame({\"sen\":all_sen_bad,\"leb\":all_leb_bad})\ndf_2 = df.groupby(\"sen\").agg(list).reset_index()\n\n# Merging the labels\nbad = []\nnew_leb = []\nnew_sen = []\ntotal_leb = ['L-PERSON', 'I-PERSON', 'L-ORG', 'B-PERSON', 'U-DATE', 'I-GPE', 'U-ORG', 'L-GPE', 'U-GPE', 'U-PERSON', 'B-GPE', 'I-ORG', 'B-LAW', 'L-LAW', 'I-LAW', 'B-ORG']\nfor i in range(len(df_2)):\n    li = df_2.iloc[i][\"leb\"]\n    new_sen.append(df_2.iloc[i][\"sen\"])\n    if len(li)==1:\n        new_leb.append(li[0])\n    else:\n        bad.append(i)\n        corr_leb = li[0]\n        dummy = [[k[j] for k in li] for j in range(len(li[0]))]\n        for t in range(len(corr_leb)):\n            for tt in range(len(li)):\n                if dummy[t][tt] in total_leb:\n                    corr_leb[t]=dummy[t][tt]\n                    \n                    break\n        \n        new_leb.append(corr_leb)\n        \n\n# declaring new data\ndata_2 = [[new_sen[i],new_leb[i]] for i in range(len(new_sen))]\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T14:14:52.466340Z","iopub.execute_input":"2023-07-22T14:14:52.466778Z","iopub.status.idle":"2023-07-22T14:14:52.526274Z","shell.execute_reply.started":"2023-07-22T14:14:52.466742Z","shell.execute_reply":"2023-07-22T14:14:52.525094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will repeat similar process as dataset-1","metadata":{}},{"cell_type":"code","source":"all_sen_2 = []\nall_leb_2 = []\nall_leb_per_2 = []\n\nfor i in range(len(data_2)):\n    sen = data_2[i][0].replace(\",\",\" ,\").replace(\"ред\",\" ред\").replace(\":\",\" :\").replace(\"-\",\" - \").replace(\"(\",\"( \").replace(\")\",\" )\").replace(\"'\",\" '\").replace(\";\",\" ;\").replace(\"  \",\" \")\n    if len(sen.split(\" \"))!=len(data_2[i][1]):\n        continue\n    all_sen_2.append(sen)\n    all_leb_2.append(data_2[i][1])\n    all_leb_per_2.append([1 if i[-6:]=='PERSON' else 0 for i in data_2[i][1]])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T14:14:56.940355Z","iopub.execute_input":"2023-07-22T14:14:56.940807Z","iopub.status.idle":"2023-07-22T14:14:56.985016Z","shell.execute_reply.started":"2023-07-22T14:14:56.940775Z","shell.execute_reply":"2023-07-22T14:14:56.983715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Full Data","metadata":{}},{"cell_type":"markdown","source":"Now we will add two datasets and shuffle all of them.","metadata":{}},{"cell_type":"code","source":"all_sen_20 = all_sen+all_sen_2\nall_leb_20 = all_leb+all_leb_2\nall_leb_per_20 = all_leb_per+all_leb_per_2\n\nimport random\nidxs = np.arange(len(all_sen_20))\nrandom.shuffle(idxs)\n\nall_sen_20 = [all_sen_20[i] for i in idxs]\nall_leb_20 = [all_leb_20[i] for i in idxs]\nall_leb_per_20 = [all_leb_per_20[i] for i in idxs]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T14:14:59.674475Z","iopub.execute_input":"2023-07-22T14:14:59.674916Z","iopub.status.idle":"2023-07-22T14:14:59.707341Z","shell.execute_reply.started":"2023-07-22T14:14:59.674880Z","shell.execute_reply":"2023-07-22T14:14:59.706352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now check if the labels and sentences are correlated with each other.","metadata":{}},{"cell_type":"code","source":"for i in range(len(all_sen_20)):\n    if (len(all_sen_20[i].split(\" \"))==len(all_leb_20[i])==len(all_leb_per_20[i]))!=True:\n        print(\"ok\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-22T14:15:02.032662Z","iopub.execute_input":"2023-07-22T14:15:02.033768Z","iopub.status.idle":"2023-07-22T14:15:02.067694Z","shell.execute_reply.started":"2023-07-22T14:15:02.033690Z","shell.execute_reply":"2023-07-22T14:15:02.066277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we will separate the examples which contain at least one name and which contain no name. It is important to make the split balanced.","metadata":{}},{"cell_type":"code","source":"all_sen_201 = [] #2612\nall_leb_201 = []\nall_leb_per_201 = []\n\nall_sen_202 = [] #6682\nall_leb_202 = []\nall_leb_per_202 = []\n\nfor i in range(len(all_sen_20)):\n    if 1 in all_leb_per_20[i]:\n        all_sen_201.append(all_sen_20[i])\n        all_leb_201.append(all_leb_20[i])\n        all_leb_per_201.append(all_leb_per_20[i])\n    else:\n        all_sen_202.append(all_sen_20[i])\n        all_leb_202.append(all_leb_20[i])\n        all_leb_per_202.append(all_leb_per_20[i])\n        ","metadata":{"execution":{"iopub.status.busy":"2023-07-22T14:15:06.824522Z","iopub.execute_input":"2023-07-22T14:15:06.824924Z","iopub.status.idle":"2023-07-22T14:15:06.849021Z","shell.execute_reply.started":"2023-07-22T14:15:06.824892Z","shell.execute_reply":"2023-07-22T14:15:06.847769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we will split the data in three parts. They are - Train, Validation and Test. After that we will save the data in json files.","metadata":{}},{"cell_type":"code","source":"train_sen = all_sen_201[:2089]*3+all_sen_202[:5345] # a multiplication factor of 3 is used to make it balanced\ntrain_leb = all_leb_201[:2089]*3+all_leb_202[:5345]\ntrain_leb_per = all_leb_per_201[:2089]*3+all_leb_per_202[:5345]\n\nvalid_sen = all_sen_201[2089:2350]+all_sen_202[5345:6013]\nvalid_leb = all_leb_201[2089:2350]+all_leb_202[5345:6013]\nvalid_leb_per = all_leb_per_201[2089:2350]+all_leb_per_202[5345:6013]\n\ntest_sen = all_sen_201[2350:]+all_sen_202[6013:]\ntest_leb = all_leb_201[2350:]+all_leb_202[6013:]\ntest_leb_per = all_leb_per_201[2350:]+all_leb_per_202[6013:]\n\ntrain_dict = {\"sentences\":train_sen, \"entity\":train_leb, \"label\":train_leb_per}\nvalid_dict = {\"sentences\":valid_sen, \"entity\":valid_leb, \"label\":valid_leb_per}\ntest_dict = {\"sentences\":test_sen, \"entity\":test_leb, \"label\":test_leb_per}\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T14:17:13.020437Z","iopub.execute_input":"2023-07-22T14:17:13.020845Z","iopub.status.idle":"2023-07-22T14:17:13.033804Z","shell.execute_reply.started":"2023-07-22T14:17:13.020814Z","shell.execute_reply":"2023-07-22T14:17:13.032263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_json(train_dict,\"train\")\nsave_json(valid_dict,\"valid\")\nsave_json(test_dict,\"test\")","metadata":{"execution":{"iopub.status.busy":"2023-07-22T14:17:24.125176Z","iopub.execute_input":"2023-07-22T14:17:24.125585Z","iopub.status.idle":"2023-07-22T14:17:24.486244Z","shell.execute_reply.started":"2023-07-22T14:17:24.125554Z","shell.execute_reply":"2023-07-22T14:17:24.484900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}