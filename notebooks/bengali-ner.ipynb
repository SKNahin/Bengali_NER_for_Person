{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport torch\nimport torch.nn as nn\nimport numpy as np  \nimport datasets\nfrom datasets import Dataset as Datasets\nfrom datasets import DatasetDict\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer,AutoTokenizer, DataCollatorForTokenClassification, logging","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2023-07-22T15:23:41.007082Z","iopub.execute_input":"2023-07-22T15:23:41.007462Z","iopub.status.idle":"2023-07-22T15:23:53.330738Z","shell.execute_reply.started":"2023-07-22T15:23:41.007431Z","shell.execute_reply":"2023-07-22T15:23:53.329758Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device=torch.device(\"cuda:0\")\n    print(\"Running on the GPU\")\n    torch.cuda.empty_cache()\n\nelse:\n    device=torch.device(\"cpu\")\n    print(\"Running on the CPU\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:23:53.332538Z","iopub.execute_input":"2023-07-22T15:23:53.332872Z","iopub.status.idle":"2023-07-22T15:23:53.364303Z","shell.execute_reply.started":"2023-07-22T15:23:53.332840Z","shell.execute_reply":"2023-07-22T15:23:53.363166Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Running on the GPU\n","output_type":"stream"}]},{"cell_type":"code","source":"# This are some functions to help in loading and saving json file\n\ndef save_json(data,name):\n    with open(name+'.json', 'w',encoding='utf8') as f:\n        json.dump(data, f,ensure_ascii=False)\n        \ndef save_jsonl(data,name):\n    with open(name+'.jsonl', 'w') as f:\n        for entry in data:\n            json.dump(entry, f)\n            outfile.write('\\n')\n\ndef load_json(path):\n    f = open (path, \"r\")\n    data = json.loads(f.read())\n    f.close()\n    return data\n\ndef load_jsonl(path):\n    f = open(path,\"r\").readlines()\n    data=[]\n    for i in f:\n        d = json.loads(i)\n        data.append(d)\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:23:53.365573Z","iopub.execute_input":"2023-07-22T15:23:53.366515Z","iopub.status.idle":"2023-07-22T15:23:53.377115Z","shell.execute_reply.started":"2023-07-22T15:23:53.366482Z","shell.execute_reply":"2023-07-22T15:23:53.376127Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"markdown","source":"The raw datasets are processed according to the need. The notebook for this is shared in another notebook. Let's load the data and make a dictionary of whole dataset. ","metadata":{}},{"cell_type":"code","source":"# Replace the derectory according to yours\ntrain_data = load_json(\"path/to/train.json\")\nvalid_data = load_json(\"path/to/valid.json\")\ntest_data = load_json(\"path/to/test.json\")\n\nall_data_dict = DatasetDict({\"train\": Datasets.from_dict(train_data),\"valid\":Datasets.from_dict(valid_data),\"test\":Datasets.from_dict(test_data)})","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:23:53.379379Z","iopub.execute_input":"2023-07-22T15:23:53.379858Z","iopub.status.idle":"2023-07-22T15:23:53.646608Z","shell.execute_reply.started":"2023-07-22T15:23:53.379822Z","shell.execute_reply":"2023-07-22T15:23:53.645531Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The dataset is prepared in such way that each spaces separated parts in a sentence will have a label. But during tokenization those parts may get splitted, so, we will have to readjust the labels according to the tokenizer with some other basic operations during preprocessing of the dataset.","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    \n    dic = {}\n    dic[\"input_ids\"] = []\n    dic['token_type_ids'] = []\n    dic['attention_mask'] = []\n    dic[\"labels\"] = []\n     \n    for i in range(len(examples[\"sentences\"])):\n        sen_li = examples[\"sentences\"][i].split(\" \")\n        tok = tokenizer(sen_li, max_length=20, truncation=True)\n\n        dic_input_ids = [2]\n        dic_token_type_ids = [0]\n        dic_attention_mask = [1]\n        dic_labels = [0]\n\n        input_ids =tok[\"input_ids\"]\n        token_type_ids =tok['token_type_ids']\n        attention_mask=tok['attention_mask']\n        label=examples['label'][i]\n\n        for t in range(len(sen_li)):\n            input_id_len = len(input_ids[t])\n            dic_input_ids.extend(input_ids[t][1:input_id_len-1])\n            dic_token_type_ids.extend(token_type_ids[t][1:input_id_len-1])\n            dic_attention_mask.extend(attention_mask[t][1:input_id_len-1])\n            dic_labels.extend([label[t]]*(input_id_len-2))\n    \n        \n        dic[\"input_ids\"].append(dic_input_ids+[3]+[0]*(256-len(dic_input_ids)-1))\n        dic['token_type_ids'].append(dic_token_type_ids+[0]*(256-len(dic_token_type_ids)))\n        dic['attention_mask'].append(dic_attention_mask+[1]+[0]*(256-len(dic_attention_mask)-1))\n        dic['labels'].append(dic_labels+[0]*(256-len(dic_labels)))\n    \n    return dic\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:23:53.647952Z","iopub.execute_input":"2023-07-22T15:23:53.648328Z","iopub.status.idle":"2023-07-22T15:23:53.660165Z","shell.execute_reply.started":"2023-07-22T15:23:53.648295Z","shell.execute_reply":"2023-07-22T15:23:53.659207Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Let's load the tokenizer first\ntokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\")\n\n# Creat the dataset according to the preprocessing\nall_dataset = all_data_dict.map(preprocess_function, batched=True)\nall_dataset_back = all_dataset.copy() # Keeping a copy\nall_dataset=all_dataset.remove_columns(['sentences','entity','label']) #Dropping unnecessary columns for training\n\n# Preparing the collator\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-07-22T15:23:53.661950Z","iopub.execute_input":"2023-07-22T15:23:53.662707Z","iopub.status.idle":"2023-07-22T15:24:08.459461Z","shell.execute_reply.started":"2023-07-22T15:23:53.662674Z","shell.execute_reply":"2023-07-22T15:24:08.458525Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"415e4dea954a4dc4afafb9bf7afd0250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"142e848c1ea74653934378a7eba59b17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/528k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"891b06b6453e4ba4bc3ceb431b02878a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc6920de6bda48aba5d987294789a4a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e56a25978e843fd9b63671defa3c146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"888978aad8b24a1ea7cf6b53809a4ae2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c3ed887ba24d89bb69ac73e736cb4e"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Modifying Trainer","metadata":{}},{"cell_type":"markdown","source":"After preparing the data the next task is to changing the trainer according to our need. In our case, we are doning Bengali NER only for Persons. As it is a binary classinfication we will have to change the loss calculation process in the trainer. Also, token's are imbalanced here. So, we will use a F1 score a our metric. For this purpose we will write another function here.","metadata":{}},{"cell_type":"code","source":"\nclass MyTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        self.my_loss = nn.BCELoss(reduction = 'none')\n        self.sigmoid = nn.Sigmoid()\n        \n    def compute_loss(self, model, inputs,return_outputs=False):\n\n        outputs = model(**inputs)\n        outputs.logits = outputs.logits[:,:,0]\n        att_mask = inputs.attention_mask.type(torch.float32) # attention mask will be used to exclude redundent portion from loss calculation\n        \n        loss = torch.sum(self.my_loss(self.sigmoid(outputs.logits),inputs.labels.type(torch.float32))*att_mask)/torch.sum(att_mask)\n        \n        return (loss, outputs) if return_outputs else loss\n    \n\ndef compute_metrics(eval_pred):\n    predictions, label_ids, inputs = eval_pred\n    pred_mask = (inputs!=2)*(inputs!=3)*(inputs!=0)*1 # This mask will be used to exclude redundent parts from calculation\n    predictions = (1/(1 + np.exp(-predictions))>0.5)*1\n\n    TP = np.sum((label_ids==predictions)*label_ids*pred_mask)\n    FP = np.sum((label_ids!=predictions)*predictions*pred_mask)\n    FN = np.sum((label_ids!=predictions)*label_ids*pred_mask)\n    \n    F1 = TP/(TP+0.5*(FP+FN)+1e-10)\n    \n    result = {}\n    result[\"F1\"]=F1\n    \n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:24:08.461741Z","iopub.execute_input":"2023-07-22T15:24:08.462206Z","iopub.status.idle":"2023-07-22T15:24:08.474001Z","shell.execute_reply.started":"2023-07-22T15:24:08.462170Z","shell.execute_reply":"2023-07-22T15:24:08.473034Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"After completing all the basic needs for training we can now start our training.","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.login(key='your wandb log in key') # logging in to wandb\n\n# Loading model\nmodel = AutoModelForTokenClassification.from_pretrained(\"csebuetnlp/banglabert\", num_labels=2)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    metric_for_best_model=\"F1\",\n    num_train_epochs=3,\n    weight_decay=0.01,\n    include_inputs_for_metrics =True,\n    seed = 0\n)\n\ntrainer = MyTrainer(\n    model=model.to(device),\n    args=training_args,\n    train_dataset=all_dataset[\"train\"],\n    eval_dataset=all_dataset[\"valid\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:24:09.907031Z","iopub.execute_input":"2023-07-22T15:24:09.907412Z","iopub.status.idle":"2023-07-22T15:24:36.452450Z","shell.execute_reply.started":"2023-07-22T15:24:09.907382Z","shell.execute_reply":"2023-07-22T15:24:36.451399Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c6ddece34a54f8da641c2bd8a841f14"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ElectraForTokenClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:24:36.454304Z","iopub.execute_input":"2023-07-22T15:24:36.455098Z","iopub.status.idle":"2023-07-22T15:40:24.756156Z","shell.execute_reply.started":"2023-07-22T15:24:36.455062Z","shell.execute_reply":"2023-07-22T15:40:24.755028Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msknahin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230722_152436-y6u49ss3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sknahin/huggingface/runs/y6u49ss3' target=\"_blank\">morning-gorge-208</a></strong> to <a href='https://wandb.ai/sknahin/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sknahin/huggingface' target=\"_blank\">https://wandb.ai/sknahin/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sknahin/huggingface/runs/y6u49ss3' target=\"_blank\">https://wandb.ai/sknahin/huggingface/runs/y6u49ss3</a>"},"metadata":{}},{"name":"stderr","text":"You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2178' max='2178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2178/2178 15:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.071900</td>\n      <td>0.035670</td>\n      <td>0.892300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.034400</td>\n      <td>0.043089</td>\n      <td>0.898200</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.014300</td>\n      <td>0.044240</td>\n      <td>0.899200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2178, training_loss=0.033961700123313156, metrics={'train_runtime': 947.9656, 'train_samples_per_second': 36.748, 'train_steps_per_second': 2.298, 'total_flos': 4551266908827648.0, 'train_loss': 0.033961700123313156, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"out = trainer.predict(all_dataset[\"test\"])\npredictions, label_ids ,metric_= out\nprint(metric_)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:40:24.757654Z","iopub.execute_input":"2023-07-22T15:40:24.758608Z","iopub.status.idle":"2023-07-22T15:40:33.098253Z","shell.execute_reply.started":"2023-07-22T15:40:24.758567Z","shell.execute_reply":"2023-07-22T15:40:33.095455Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'test_loss': 0.04884174093604088, 'test_F1': 0.8852, 'test_runtime': 8.3207, 'test_samples_per_second': 111.889, 'test_steps_per_second': 7.091}\n","output_type":"stream"}]},{"cell_type":"code","source":"all_sentences = all_dataset_back[\"test\"][\"sentences\"]\nall_inputs = np.array(all_dataset_back[\"test\"][\"input_ids\"])\nall_labels = np.array(all_dataset_back[\"test\"][\"labels\"])\nall_masks = (all_inputs!=0)*(all_inputs!=2)*(all_inputs!=3)*1\npredictions = (1/(1 + np.exp(-predictions))>0.5)*all_masks","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:40:33.102522Z","iopub.execute_input":"2023-07-22T15:40:33.103407Z","iopub.status.idle":"2023-07-22T15:40:33.488708Z","shell.execute_reply.started":"2023-07-22T15:40:33.103370Z","shell.execute_reply":"2023-07-22T15:40:33.485948Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# The code bellow will generate a dataframe of predictions and groundtruths. \n# It is tried to keep the names separated with a comma if there are two or more names in a sentence\n\nimport pandas as pd\n\nall_sen = []\nall_name = []\nall_name_leb = []\n\nfor i in range(len(predictions)):\n    sen = all_sentences[i]\n    name_tok_idx = np.where(predictions[i]==1)[0]\n    name_tok0 = all_inputs[i][name_tok_idx]\n    if len(name_tok_idx)!=0:\n        name_tok = [name_tok0[0]]\n        for k in range(len(name_tok_idx)-1):\n            if name_tok_idx[k+1]-name_tok_idx[k]!=1:\n                name_tok.extend([16])\n            else:\n                name_tok.extend([name_tok0[k+1]])\n    else:\n        name_tok = []\n    name = tokenizer.decode(name_tok,skip_special_tokens=True)\n    \n    name_leb_tok_idx = np.where(all_labels[i]==1)[0]\n    name_leb_tok0 = all_inputs[i][name_leb_tok_idx]\n    if len(name_leb_tok_idx)!=0:\n        name_leb_tok = [name_leb_tok0[0]]\n        for k in range(len(name_leb_tok_idx)-1):\n            if name_leb_tok_idx[k+1]-name_leb_tok_idx[k]!=1:\n                name_leb_tok.extend([16])\n            else:\n                name_leb_tok.extend([name_leb_tok0[k+1]])\n    else:\n        name_leb_tok = []\n        \n    name_leb = tokenizer.decode(name_leb_tok,skip_special_tokens=True)\n    \n    all_sen.append(sen)\n    all_name.append(name)\n    all_name_leb.append(name_leb)\n\ndf = pd.DataFrame({\"sentence\":all_sen,\"pred_name\":all_name,\"gt_name\":all_name_leb})","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:40:33.490231Z","iopub.execute_input":"2023-07-22T15:40:33.490834Z","iopub.status.idle":"2023-07-22T15:40:33.562589Z","shell.execute_reply.started":"2023-07-22T15:40:33.490801Z","shell.execute_reply":"2023-07-22T15:40:33.561421Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df[0:100]","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:40:33.563941Z","iopub.execute_input":"2023-07-22T15:40:33.564370Z","iopub.status.idle":"2023-07-22T15:40:33.592059Z","shell.execute_reply.started":"2023-07-22T15:40:33.564306Z","shell.execute_reply":"2023-07-22T15:40:33.590976Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                             sentence  \\\n0   সিলেটের উত্তর সার্কেলের সহকারী পুলিশ সুপার ( এ...   \n1   এরকম একজন হারিয়ে যাওয়া মনীষী কিশোরগঞ্জের আনন্দ...   \n2   একতরফা এই নির্বাচন করতে না পারলে শক্তির খেলায় ...   \n3   ভারতের কূটনীতিক দেবযানী খোবরাগাড়েকে জাতিসংঘে ব...   \n4   বিপ্রবেলঘরিয়া ইউনিয়ন আওয়ামী লীগের সভাপতি হে...   \n..                                                ...   \n95  চট্টগ্রাম বিশ্ববিদ্যালয়ের সমাজতত্ত্ব বিভাগের ...   \n96  প্রশাসনিক এই সংকটের মধ্যেই এএপির অভ্যন্তরীণ সং...   \n97  ইত্তেফাক-এর মাঈনুল আলম ও ভোরের কাগজ-এর আঙ্গুর ...   \n98  উপমহাদেশের প্রখ্যাত নৃত্যগুরু প্রয়াত বুলবুল চৌ...   \n99  অন্য যে চারজনের বিরুদ্ধে গ্রেপ্তারি পরোয়ানা জা...   \n\n                                        pred_name  \\\n0      মহিউদ্দিন সুহেল,. শফিকুর রহমান খান, আমীনসহ   \n1                                   আনন্দমোহন বসু   \n2                               শমসের মবিন চৌধুরী   \n3                                         দেবযানী   \n4                             হেলাল উদ্দীন, হোসেন   \n..                                            ...   \n95                               ইফতেখার উদ্দিনকে   \n96                          জিতেন্দ্র সিং টোমারকে   \n97                            মাঈনুল আলম,ুর নাহার   \n98                         বুলবুল চৌধুরীর, চৌধুরী   \n99  এম মাহাবুবুল মোর্শেদ, রাজ্জাক,ুজ্জামান, হোসেন   \n\n                                          gt_name  \n0      মহিউদ্দিন সুহেল,. শফিকুর রহমান খান, আমীনসহ  \n1                                   আনন্দমোহন বসু  \n2                               শমসের মবিন চৌধুরী  \n3                                         দেবযানী  \n4                             হেলাল উদ্দীন, হোসেন  \n..                                            ...  \n95                       অধ্যাপক ইফতেখার উদ্দিনকে  \n96                          জিতেন্দ্র সিং টোমারকে  \n97                            মাঈনুল আলম,ুর নাহার  \n98                         বুলবুল চৌধুরীর, চৌধুরী  \n99  এম মাহাবুবুল মোর্শেদ, রাজ্জাক,ুজ্জামান, হোসেন  \n\n[100 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>pred_name</th>\n      <th>gt_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>সিলেটের উত্তর সার্কেলের সহকারী পুলিশ সুপার ( এ...</td>\n      <td>মহিউদ্দিন সুহেল,. শফিকুর রহমান খান, আমীনসহ</td>\n      <td>মহিউদ্দিন সুহেল,. শফিকুর রহমান খান, আমীনসহ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>এরকম একজন হারিয়ে যাওয়া মনীষী কিশোরগঞ্জের আনন্দ...</td>\n      <td>আনন্দমোহন বসু</td>\n      <td>আনন্দমোহন বসু</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>একতরফা এই নির্বাচন করতে না পারলে শক্তির খেলায় ...</td>\n      <td>শমসের মবিন চৌধুরী</td>\n      <td>শমসের মবিন চৌধুরী</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ভারতের কূটনীতিক দেবযানী খোবরাগাড়েকে জাতিসংঘে ব...</td>\n      <td>দেবযানী</td>\n      <td>দেবযানী</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>বিপ্রবেলঘরিয়া ইউনিয়ন আওয়ামী লীগের সভাপতি হে...</td>\n      <td>হেলাল উদ্দীন, হোসেন</td>\n      <td>হেলাল উদ্দীন, হোসেন</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>চট্টগ্রাম বিশ্ববিদ্যালয়ের সমাজতত্ত্ব বিভাগের ...</td>\n      <td>ইফতেখার উদ্দিনকে</td>\n      <td>অধ্যাপক ইফতেখার উদ্দিনকে</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>প্রশাসনিক এই সংকটের মধ্যেই এএপির অভ্যন্তরীণ সং...</td>\n      <td>জিতেন্দ্র সিং টোমারকে</td>\n      <td>জিতেন্দ্র সিং টোমারকে</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>ইত্তেফাক-এর মাঈনুল আলম ও ভোরের কাগজ-এর আঙ্গুর ...</td>\n      <td>মাঈনুল আলম,ুর নাহার</td>\n      <td>মাঈনুল আলম,ুর নাহার</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>উপমহাদেশের প্রখ্যাত নৃত্যগুরু প্রয়াত বুলবুল চৌ...</td>\n      <td>বুলবুল চৌধুরীর, চৌধুরী</td>\n      <td>বুলবুল চৌধুরীর, চৌধুরী</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>অন্য যে চারজনের বিরুদ্ধে গ্রেপ্তারি পরোয়ানা জা...</td>\n      <td>এম মাহাবুবুল মোর্শেদ, রাজ্জাক,ুজ্জামান, হোসেন</td>\n      <td>এম মাহাবুবুল মোর্শেদ, রাজ্জাক,ুজ্জামান, হোসেন</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.to_csv(\"prediction.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T15:40:33.593496Z","iopub.execute_input":"2023-07-22T15:40:33.594021Z","iopub.status.idle":"2023-07-22T15:40:33.617815Z","shell.execute_reply.started":"2023-07-22T15:40:33.593984Z","shell.execute_reply":"2023-07-22T15:40:33.616761Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}